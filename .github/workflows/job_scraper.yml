name: Job Scraper

on:
  schedule:
    - cron: '0 2 * * *'  # Run daily at 2 AM UTC
  workflow_dispatch:      # Allow manual trigger

# 设置适当的权限
permissions:
  contents: write
  actions: read

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # 添加超时限制，避免无限运行
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        fetch-depth: 0  # 获取完整历史以便正确提交
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'  # 缓存依赖项以加速构建
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
      
    - name: Create data directory if not exists
      run: mkdir -p data
        
    - name: Run job scraper with debug information
      run: |
        # 显示工作目录
        pwd
        ls -la
        
        # 运行爬虫并保存日志
        python job_scraper.py 2>&1 | tee scraper_log.txt
        
        # 显示目录结构以确认文件创建
        ls -la data/ || echo "Data directory empty or not found"
      
    - name: Commit and push changes
      run: |
        # 配置Git
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "GitHub Actions"
        
        # 确保所有变更都被添加
        git add -A || true
        
        # 提交并推送
        if git commit -m "Update job listings: $(date +'%Y-%m-%d')"; then
          echo "Changes committed, pushing now..."
          git push
        else
          echo "No changes to commit"
        fi
      
    # 移除有问题的日志上传步骤，使用更简单的方法来处理输出
    - name: Check for generated files
      run: |
        echo "====== Job Scraper Results ======"
        echo "Scraper log:"
        cat scraper_log.txt || echo "No log file found"
        echo "Generated files:"
        ls -la data/ || echo "No data files found"
